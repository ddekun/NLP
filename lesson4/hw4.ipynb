{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY/knX9ScBIinXfRzByM0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddekun/NLP/blob/lesson4/lesson4/hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Введение в обработку естественного языка"
      ],
      "metadata": {
        "id": "l5SL33UgXYP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Урок 4. Тематическое моделирование. EM-алгоритм"
      ],
      "metadata": {
        "id": "isLad30TXalG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "продолжаем исследование датасета с твитами\n",
        "\n",
        "Скачиваем датасет (источник): положительные, отрицательные.\n",
        "\n",
        "рабочие ссылки с твитами https://disk.yandex.ru/i/v5HM-ENiGXZVpQ https://disk.yandex.ru/i/koR5eMCToCZS2Q\n",
        "\n",
        "как альтернатива можно скачать данные из Роспотребнадзора https://zpp.rospotrebnadzor.ru/Forum/Appeals для этого берём ноутбук parse_rospotrebnadzor.ipynb устанавливаем количество скачанных страниц больше не 50-сят хотябы 500 и для анализа берём только вопросы так как ответы есть не всегда\n",
        "\n",
        "что надо сделать\n",
        "\n",
        "объединить в одну выборку (это только для твитов), для роспотребнадзора сформировать датасет из вопросов провести исследование и выявить тематики о которых говорят в твитах (для твитов), а для роспотребнадзора так же выявить тематики о которых люди пишут проанализировать сделать визуализацию кластеров тематик проинтерпритировать получившиеся тематики"
      ],
      "metadata": {
        "id": "s0sbUYssXneG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2htV24PXWi4"
      },
      "outputs": [],
      "source": [
        "! pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim"
      ],
      "metadata": {
        "id": "zj8w-QCKXr3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "hfI2KwvFXuF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_ru')\n",
        "import pymorphy2\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm.notebook import tqdm\n",
        "from multiprocessing import Pool\n",
        "from pymystem3 import Mystem\n",
        "from gensim.models import *\n",
        "from gensim import corpora\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "QfLhgr1cXvhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive = pd.read_csv('/content/positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative = pd.read_csv('/content/negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "data = positive.append(negative)"
      ],
      "metadata": {
        "id": "JQgWM5CjXxYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "JfOKuco0XzUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "0HXGOwBaX1MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_regex = re.compile('\\w+')\n",
        "\n",
        "def find_words(text, regex = words_regex):\n",
        "    tokens =  regex.findall(text.lower())\n",
        "    return [w for w in tokens if w.isalpha() and len(w) >= 3]\n",
        "\n",
        "\n",
        "stopwords_list = stopwords.words('russian')\n",
        "\n",
        "\n",
        "add_stop_words = ['ахах', 'мочь', 'супер', 'хотяб', 'ахахаххах', 'аахха', 'настюха',\n",
        "                  'оля', 'блин']\n",
        "stopwords_list += add_stop_words\n",
        "\n",
        "\n",
        "\n",
        "def lemmatize(words, lemmer = morph, stopwords = stopwords_list):\n",
        "    lemmas = [lemmer.parse(w)[0].normal_form for w in words]\n",
        "    return [w for w in lemmas if not w in stopwords\n",
        "            and w.isalpha()]\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    result = (lemmatize(find_words(text)))\n",
        "    return nltk.pos_tag(result, lang='rus')"
      ],
      "metadata": {
        "id": "EKLYw4LvX2rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.text.iloc[10]"
      ],
      "metadata": {
        "id": "mD_GBMPaX4mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocess(data.text.iloc[10]))"
      ],
      "metadata": {
        "id": "6NnOtCUKX56b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = list(tqdm(map(preprocess, data.text), total=len(data)))"
      ],
      "metadata": {
        "id": "JAMuFKIIX533"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text[:5]"
      ],
      "metadata": {
        "id": "PHIjklGSX9Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def only_nouns(preprocessed_text):\n",
        "    token_list = []\n",
        "    for tweet in preprocessed_text:\n",
        "        token_list.append([w[0] for w in tweet if w[1]=='S'])\n",
        "    return token_list"
      ],
      "metadata": {
        "id": "IScCcTJBX-y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = only_nouns(preprocessed_text=preprocessed_text)\n",
        "data.sample(5)"
      ],
      "metadata": {
        "id": "smggItl2YBal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['text'].map(lambda d: len(d)) > 2]\n",
        "data.head()"
      ],
      "metadata": {
        "id": "sMquLX95YB_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(data['text'])\n",
        "\n",
        "dictionary.filter_extremes(no_below = 10, no_above = 0.9, keep_n=None) # игнорируем слова, которые встречаются реже 10 раз или составляют более 0.9 словаря\n",
        "dictionary.save('tweet.dict')"
      ],
      "metadata": {
        "id": "4H-cw3UpYEw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in data['text']]\n",
        "corpora.MmCorpus.serialize('tweet.model', corpus)"
      ],
      "metadata": {
        "id": "mvG4wXvHYGcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=3, chunksize=200, update_every=1, passes=2)"
      ],
      "metadata": {
        "id": "My8lfyXtYH0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda.show_topics(num_topics=5, num_words=10, formatted=True)"
      ],
      "metadata": {
        "id": "2-0hRXCtYJXA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}